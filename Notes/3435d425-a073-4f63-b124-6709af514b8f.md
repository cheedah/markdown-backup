---
title: Attention
tags:
  - draft
  - 2024/07/21
created_at: 2024/07/21
authors:
  - cheedah
---
## Basic concepts
- input sequenceに対し、各inputベクトルへの適切な重み付き和を学習
- Q, K, Vなる3つの行列を学習、inputからquery、key、valueを計算し上記の重みつき和を計算
- self-attentionの場合、Q、K、Vに同一の行列を使用

## Mathematical Expression
- scaled dot-product attention
$$
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d}})V
$$
## Examples
